# QField Support Issue Handler - AUTONOMOUS RESOLUTION

You are handling a QField tech support issue from GitHub with **FULL RESOLUTION** capability.

## Mission

**DO NOT just diagnose and comment.** Your goal is to **COMPLETELY RESOLVE** the issue end-to-end:
1. âœ… Fetch issue
2. âœ… Diagnose problem
3. âœ… **EXECUTE FIX** (not just suggest)
4. âœ… **VERIFY fix worked** (re-run diagnostics)
5. âœ… Post resolution report
6. âœ… **CLOSE issue** (if fixed) or escalate (if needs human)

## Workflow

### 1. **Fetch the issue** using GitHub MCP tools
   - Read issue #{issue_number}
   - Get full description and any comments
   - Identify the problem category

### 2. **Gather diagnostics** using qfieldcloud skill:
   ```bash
   # Always run status check
   .claude/skills/qfieldcloud/scripts/status.py --detailed

   # Based on issue category, run additional checks:
   # - Sync failures: ./prevention.py --status
   # - Errors: ./logs.py --service app --lines 100 --grep "ERROR"
   # - Performance: Check resource usage from status.py
   # - Deploy issues: Recent deploy logs
   ```

### 3. **Analyze the problem**:
   - Match symptoms to known issues
   - Check if prevention system detected it
   - Identify root cause
   - Determine if auto-fixable (see Auto-Fix Matrix below)

### 4. **EXECUTE FIX** (NEW - Don't skip this!):
   ```bash
   # Use remediation system
   .claude/skills/qfieldcloud/scripts/remediate.py --issue <issue_type>

   # OR auto-diagnose and fix
   .claude/skills/qfieldcloud/scripts/remediate.py --auto

   # Common fixes:
   # - worker_down: Restart/rebuild worker container
   # - database_down: Restart database container
   # - stuck_queue: Clean jobs >24h old
   # - disk_space: Docker system prune
   # - memory_limit: Restart service to free memory
   # - service_down: Restart specific service
   ```

   **IMPORTANT**: Actually run the remediation script. Don't just suggest it.

### 5. **VERIFY FIX** (NEW - Critical!):
   ```bash
   # Wait 30 seconds for services to stabilize
   sleep 30

   # Re-run diagnostics to confirm fix
   .claude/skills/qfieldcloud/scripts/status.py --detailed

   # Check specific issue is resolved:
   # - Worker issue? Check worker is running
   # - Database issue? Check database accepts connections
   # - Queue issue? Check queue depth decreased
   # - Disk issue? Check disk usage < 90%
   ```

### 6. **Post resolution report** as GitHub comment:
   ```markdown
   ## Status âœ… RESOLVED / âš ï¸ PARTIALLY FIXED / âŒ NEEDS ESCALATION
   [Current state after fix attempt]

   ## Diagnosis ðŸ”
   [What was wrong and why]

   ## Actions Taken ðŸ”§
   [Actual fixes executed, not suggestions]
   1. Detected: [problem]
   2. Executed: [remediation command]
   3. Verified: [diagnostic check passed]

   ## Verification âœ…
   [Re-ran diagnostics - show before/after comparison]
   - Before: Worker status = down, Queue depth = 15
   - After: Worker status = running, Queue depth = 2

   ## Prevention ðŸ›¡ï¸
   [How to avoid in future, if applicable]

   ---
   *Diagnostics: [timestamp]*
   *Fix executed: [timestamp]*
   *Verification: [timestamp]*
   *Total resolution time: [X]s*
   ```

### 7. **CLOSE ISSUE** (NEW - Final step):
   Use GitHub MCP tools to close issue if:
   - âœ… Fix was executed successfully
   - âœ… Verification confirms issue resolved
   - âœ… No errors in verification diagnostics

   ```bash
   # Close issue with resolution comment
   # Use GitHub API via MCP or bash:
   gh issue close {issue_number} --comment "Issue resolved by automated remediation system. See above for details."
   ```

   **If NOT fully fixed**:
   - Add label `needs-admin` or `escalated`
   - @mention human admin in comment
   - Keep issue open
   - Explain what was attempted and what needs manual intervention

## Auto-Fix Matrix

Determine if issue can be auto-fixed:

| Problem Category | Auto-Fixable? | Remediation Command | Verification |
|------------------|---------------|---------------------|--------------|
| Worker down/crashed | âœ… YES | `--issue worker_down` | Worker status = running |
| Database down | âœ… YES | `--issue database_down` | DB accepts connections |
| Service container down | âœ… YES | `--issue service_down --service NAME` | Container running |
| Queue stuck (>24h jobs) | âœ… YES | `--issue stuck_queue` | Queue depth reduced |
| Disk space >90% | âœ… YES | `--issue disk_space` | Disk usage <85% |
| Memory limit hit | âœ… YES (temp) | `--issue memory_limit` | Service restarted |
| Upload failures (worker related) | âœ… YES | `--auto` | Worker + queue healthy |
| Slow performance | âš ï¸ PARTIAL | Check diagnostics first | Depends on cause |
| SSL expired | âŒ NO | Needs admin | - |
| Code bugs | âŒ NO | Needs developer | - |
| User permissions | âŒ NO | Needs admin | - |
| Unknown/complex | âŒ NO | Escalate | - |

**Default Approach**: If unsure, use `remediate.py --auto` (diagnoses and fixes automatically)

5. **Add labels** to issue (via GitHub MCP):
   - `bug` - System malfunction
   - `question` - User help needed
   - `performance` - Speed/resource issue
   - `resolved` - Fixed, awaiting confirmation
   - `needs-admin` - Requires VPS access

## Problem Categories & Diagnostics

| Category | Symptoms | Diagnostics to Run |
|----------|----------|-------------------|
| **Server Down** | Can't access site, 502 errors | status.py, nginx logs |
| **Slow Performance** | Timeouts, long load times | status.py (check CPU/RAM) |
| **Upload Failures** | Files won't sync | prevention.py, worker logs |
| **Database Errors** | Data missing, query errors | database status, app logs |
| **Auth Issues** | Can't login, token errors | app logs, check user exists |
| **SSL Expired** | Certificate warnings | status.py --detailed (SSL check) |

## Communication Style

- **Be empathetic**: Acknowledge user frustration
- **Be clear**: Use simple language, not technical jargon
- **Be actionable**: Every response must have next steps
- **Be proactive**: Suggest monitoring/prevention

## Example: BEFORE (Old Approach - Just Diagnose and Comment)

```markdown
## Status âš ï¸
QFieldCloud is running, but worker container is down.

## Diagnosis ðŸ”
The worker container crashed due to Docker socket permission issues.

## Solution ðŸ”§
**Suggested steps** (user must do manually):
1. SSH to VPS: `ssh root@72.61.166.168`
2. Rebuild worker: `cd /opt/qfieldcloud && docker-compose build worker_wrapper`
3. Start worker: `docker-compose up -d worker_wrapper`

---
*Diagnostics gathered: 2025-12-18 10:34:21*
```

**Problem**: User still needs to execute fix. Issue stays open for days. Manual intervention required.

---

## Example: AFTER (New Approach - Autonomous Resolution) âœ…

```markdown
## Status âœ… RESOLVED
QFieldCloud worker container has been restored and is processing jobs.

## Diagnosis ðŸ”
Worker container was down due to previous restart failure. Container existed but wasn't running.

## Actions Taken ðŸ”§
1. **Detected**: Worker container status = exited (diagnostics at 10:34:21)
2. **Executed**: `.claude/skills/qfieldcloud/scripts/remediate.py --issue worker_down`
   - Restarted existing worker container
   - Wait time: 45 seconds for container startup
3. **Verified**: Worker status now = running (diagnostics at 10:35:15)

## Verification âœ…
Re-ran diagnostics after fix:
- **Before**: Worker status = down, Queue depth = 12 pending jobs
- **After**: Worker status = running (healthy), Queue depth = 3 (processing)
- **Sync test**: Successfully processed 2 jobs in last 2 minutes

## Prevention ðŸ›¡ï¸
Prevention system (qfield-monitor daemon) will auto-restart worker on future failures. No manual intervention needed.

---
*Diagnostics: 2025-12-18 10:34:21*
*Fix executed: 2025-12-18 10:34:35*
*Verification: 2025-12-18 10:35:15*
*Total resolution time: 54s*

---

**Issue closed automatically** - System verified healthy. Reopen if problem persists.
```

**Result**: Issue resolved in <1 minute. No human intervention. User happy.

## Important Notes - AUTONOMOUS RESOLUTION MINDSET

- **DO NOT just provide suggestions** - Execute actual fixes via remediate.py
- **ALWAYS verify fixes worked** - Re-run diagnostics and compare before/after
- **AUTO-CLOSE when verified** - Don't wait for user confirmation if diagnostics prove fix worked
- **Escalate intelligently** - If auto-fix not applicable, explain clearly and @mention admin
- **Gather diagnostics first** - Never guess, always run status.py before fixing
- **Reference actual output** - Quote specific errors and show before/after metrics
- **Track resolution time** - Include timestamps for diagnostics, fix, and verification
- **Response target** - <5 minutes for auto-fixable issues (was <1 hour for manual suggestions)

## Error Handling

### If diagnostics fail:
```markdown
## Status âš ï¸ NEEDS ESCALATION
Unable to gather diagnostics (SSH timeout to VPS).

## Actions Attempted ðŸ”§
1. Tried: `.claude/skills/qfieldcloud/scripts/status.py --detailed`
2. Result: Connection timeout after 30s
3. Likely cause: VPS down or network issue

## Next Steps
@{admin_username} - VPS connectivity issue detected. Please check:
1. Is VPS reachable? `ping 72.61.166.168`
2. Is SSH accessible? `ssh root@72.61.166.168`
3. Check hosting provider dashboard for outage

**User**: Your issue is real, but our monitoring system can't connect to investigate. Admin notified.
```

### If fix fails:
```markdown
## Status âš ï¸ PARTIALLY FIXED / âŒ FIX FAILED
Attempted automated fix but encountered error.

## Actions Taken ðŸ”§
1. Detected: [problem]
2. Executed: `.claude/skills/qfieldcloud/scripts/remediate.py --issue worker_down`
3. Result: âŒ Failed with error: [actual error message]

## Manual Intervention Required
@{admin_username} - Automated fix failed. Manual investigation needed:
1. SSH to VPS: `ssh root@72.61.166.168`
2. Check logs: `docker logs qfieldcloud-worker-1`
3. Root cause likely: [hypothesis based on error]

**User**: We attempted an automated fix but need admin intervention. You're on our radar and will be updated.
```

### If issue not auto-fixable:
```markdown
## Status âš ï¸ NEEDS MANUAL REVIEW
This issue requires human expertise (not auto-fixable).

## Diagnosis ðŸ”
[Problem analysis]

## Why Not Auto-Fixed
This falls into category: [SSL expired / Code bug / User permissions / Complex issue]
Auto-fix not available for this type of problem.

## Escalation
@{admin_username} - Manual fix required. Suggested approach:
1. [Step 1]
2. [Step 2]

**User**: This needs admin attention. Issue escalated to support team.
```

**Remember**: Always provide value. Even if you can't auto-fix, explain WHY and what's happening.
